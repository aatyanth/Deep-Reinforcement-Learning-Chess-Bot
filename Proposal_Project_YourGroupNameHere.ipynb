{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Sungwook Min\n",
    "- Aatyanth Thimma-Udayakumar\n",
    "- Vu Le\n",
    "- Haoyan Wan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "For our final project, we aim to develop a chess-playing AI using Deep Reinforcement Learning with an attention-based transformer architecture, a reinforcement learning (RL) algorithm commonly used for continuous decision-making in complex environments like chess. The goal is to train an agent capable of making optimal chess moves based on learned strategies through trial and error rather than pre-programmed heuristics. Our chess bot will interact with an environment based on gym-chess to improve through self-play and reinforcement learning. Monte Carlo Tree Search (MCTS) will be integrated into the training pipeline to improve decision-making and move selection by balancing exploration and exploitation. Success will be measured using multiple performance metrics, including decision efficiency in game situations, evaluation of how powerful a certain move was, and of course, overall win-loss ratio. The Elo rating system will serve as our primary evaluation metric, where our chess bot will play against known chess engines of varying Elo. Through model tuning and iterative training, we hope to develop a competitive chess bot that is capable of making intelligent, high-quality moves."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "[<sup>[1]</sup>]()\n",
    "The paper \"Acquisition of Chess Knowledge in AlphaZero\" explores how AlphaZero, a self-learning chess engine, learns through self-play, without any human guidance[<sup>[1]</sup>](). Unlike Stockfish, which relies on predefined heuristics and brute-force search, AlphaZero combines Monte Carlo Tree Search (MCTS) with a deep neural network to evaluate positions dynamically[<sup>[1]</sup>](). It develops concepts such as piece values, mobility, king safety, and material advantage, which emerge naturally in its decision-making process. Initially playing random moves, AlphaZero refines its strategies between 25k to 60k training steps, finding optimal openings and improving its endgame play[<sup>[1]</sup>](). AlphaZero optimizes moves purely based on effectiveness, resulting in a highly efficient playing style. However, it's performance deteriorates on lower-end gpus. The study highlights how AlphaZero surpasses traditional engines like Stockfish by generalizing chess knowledge rather than relying on static rules, raising broader implications for AI learning and decision-making beyond chess. \n",
    "______________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "[<sup>[2]</sup>]()Furthermore, the paper, \"Chess Moves Prediction using Deep Learning Neural Networks\", examines the use of CNNs to predict chess moves, training on 1.5 million board states. While the model achieved 39.16% accuracy, it struggled with long-term planning and lost 95% of games to Stockfish, indicating that CNNs alone are insufficient for strong chess AI[<sup>[2]</sup>](). Unlike AlphaZero, which integrates CNNs with Monte Carlo Tree Search (MCTS) and reinforcement learning, this model relied on supervised learning, limiting its adaptability and generalization beyond its training data[<sup>[2]</sup>](). The study highlights that while CNNs can recognize positional and tactical patterns, they lack the deep search capability needed for high-level play. The model showed a tendency to prioritize piece activity over long-term positional advantages, sometimes sacrificing material without fully evaluating the consequences[<sup>[2]</sup>](). With the absence of a reinforcement learning component meant it could not self-improve through gameplay. These findings suggest that PNN + MCTS could be a viable alternative, potentially offering a more computationally efficient yet effective approach to chess AI by combining PNNs' lightweight evaluation with MCTS' deep search capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "How does a reinforcement learning-based chess engine, trained solely through self-play without prior human knowledge, compare in performance and decision-making efficiency to traditional search-based engines like Stockfish and hybrid engines like Leela Chess Zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "For this project, we will not be using any external data sets as we seek to train the model on Reinforcement Learning (RL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We propose a solution similar to that of AlphaZero using Deep Reinforcement Learning. Our approach involves creating a Policy Neural Network (Policy NN) that takes in the board state as input and produces two outputs: (move_distribution, win_percentage). The unique aspect of our implementation is that the Policy NN will be built using an attention-based transformer architecture instead of traditional convolutional networks. The output head will consist of an 8×8×73 tensor, representing all possible moves in chess.\n",
    "\n",
    "To train this network, we will set up a reinforcement learning environment using OpenAI Gym. We will create self-play agents that use the probabilities generated by the Policy NN, combined with Monte Carlo Tree Search (MCTS), to play games against themselves. At each step of the game, the agent will record the current board state, the move distribution obtained from MCTS, and the final win/loss outcome. The MCTS component ensures that move selection is biased for less explored branches, striking a balance between exploration and exploitation through mechanisms such as the PUCT (Predictor + Upper Confidence Bound for Trees) formula.\n",
    "\n",
    "After each self-play game, we will compute the loss function, which consists of three key terms:\n",
    "\n",
    "Mean Squared Error (MSE) loss for win percentage prediction, ensuring that the value function correctly estimates the probability of winning from a given board state.\n",
    "Cross-Entropy Loss between the MCTS move distribution and the policy network’s move distribution, refining the policy network to match the improved move selection of MCTS.\n",
    "A regularization term to prevent overfitting and stabilize training.\n",
    "The loss will be backpropagated through gradient descent, updating the Policy NN weights. This process will repeat continuously, with the updated policy network generating improved move distributions, leading to stronger self-play agents over time.\n",
    "\n",
    "This iterative cycle of self-play, MCTS-guided move selection, and policy refinement ensures that the model improves progressively, discovering stronger moves while maintaining a balance between exploration (searching for new strategies) and exploitation (using known strong moves effectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "\n",
    "Elo Rating System (Primary Metric)\n",
    "\n",
    "We will use the standard Chess ELO rating system which is the standard in measuring chess playing strength. \n",
    "The ELO derivation will be made through repeated matches with known chess engines of various ratings. \n",
    "We will derive the ELO using well known equations and from tests.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Informed Consent**\n",
    "  * We will make sure to inform the owner of the gym-chess environment and the owner of the chess play data about the use of their code and gain persmission to use their source code.\n",
    "* **Data Security**\n",
    "  * Since the owner of the gym-chess environment and chess play data may not want public display of their source code in a third-party's repository, we will make sure to keep the data secure and make our repository private, only granting access to appropriate individuals.\n",
    "* **Data Storage**\n",
    "  * We plan to delete any data that we have collected for this project after the conclusion of the project.\n",
    "* **Data Representation**\n",
    "  * While cleaning and ensuring the data can be used for our project, we will make sure not to implement any new policies, inputs, or bias to represent the data in the best way possible.\n",
    "* **Fair Play**\n",
    "  * We will not produce the chess bot with the intent to bypass any anti-cheating mechanisms and will work to the best of our abilities to ensure the chess bot is not used for any misuse.\n",
    "* **AI Bias**\n",
    "  * We will train the chess bot to have a dynamic style of play and ensure that it doesn't develop a preference for a certain style of play.\n",
    "* **Interpretability**\n",
    "  * We will make sure to use appropriate techniques and visualizations, if applicable, to explain our model's decision making process to the best of our ability.\n",
    "* **Auditability**\n",
    "  * We will make sure that all code, data, visualizations, and results produced through our project will be reasonable and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Team Expectation 1: Communication**\n",
    "  * We will use the designated group chat for projecet-related communication and be responsible to the best of our abilities to meet all project guidelines, team expecatations, and individual responsibilities.\n",
    "  * If a group member is unable to attend a meeting or unable to complete an assigned duty by the deadline set by the group, he/she will inform the rest of the group as soon as possible.\n",
    "* **Team Expectation 2: Time Management**\n",
    "  * All group members will complete their assigned work by the deadline agreed by the group and will work ahead of time to ensure all deadlines are met.\n",
    "  * All group members will make sure they have no conflicting schedules with the designated group meeting hours.\n",
    "* **Team Expecation 3: Contribution**\n",
    "  * All group members will ensure that work is divided equally and that all members will work equally hard.\n",
    "* **Team Expecation 4: Review Before Submission**\n",
    "  * All group members will review the entire draft before submission and work will only be submitted after the approval of all members.\n",
    "* **Team Expecation 5: Responsibility**\n",
    "  * If a group member fails to complete their assigned parts until the deadline, he/she will receive a warning internally. \n",
    "  * If the same group member fails to comply to the deadline again and commit their time to the project, the professor will be notified.\n",
    "  * If any group member is facing difficulty completing their assigned section, it is their responsibility to ask another group member or attend office hours for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a tentative schedule, subject to change.*\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/13  |  6:30 PM |  Brainstorm topics/problems; Determine best day and time for team meeting | Discuss and decide on project topic; Research background related to topic; Work on project proposal; Turn in proposal | \n",
    "| 2/20  |  6:30 PM |  Do additional background research on topic | Discuss ideal ML techniques for topic | \n",
    "| 2/27  | 6:30 PM  |  Examine problem environment/Clean data | Discuss structure of proposed code solution/Discuss how data wrangling for dataset |\n",
    "| 3/6  | 6:30 PM  |  Code the proposed solution/Do EDA on data | Review solution code together; Discuss data analysis plan; Start drafting Project  |\n",
    "| 3/13  | 6:30 PM  | Continue drafting project; Include visualizations for data |  Assign project sections to each member |\n",
    "| 3/17  | 6:30 PM  | Complete analysis; Complete project; Read drafted project and prepare feedback for each section | Review feedback and make final changes |\n",
    "| 3/19  | Before 11:59 PM  | N/A | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Thomas McGrath, Andrei Kapishnikov, Nenad Tomašev, Adam Pearce, Martin Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik\"></a>1.[^](#McGrath): McGrath, Thomas et. al., (2022) Acquisition of chess knowledge in AlphaZero. *Proceedings of the National Academy of Sciences (PNAS)*. https://www.pnas.org/doi/epub/10.1073/pnas.2206625119<br> \n",
    "<a name=\"Hitanshu Panchal, Siddhant Mishra, and Varsha Shrivastava\"></a>2.[^](#lorenz): Panchal, Hitanshu et. al., (2021) Chess Moves Prediction using Deep Learning Neural Networks. *International Conference on Advances in Computing and Communications (ICACC)*. https://ieeexplore.ieee.org/abstract/document/9708405<br> \n",
    "\n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
