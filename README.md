# Deep-Reinforcement-Learning-Chess-Bot

In this project, we aim to develop a chess-playing model using Deep Reinforcement Learning (DRL) with an attention-based transformer architecture, a novel variant of RL that has proven effective in continuous decision-making tasks like chess. The goal is to train an agent capable of making optimal chess moves based on learned strategies through self play using Monte Carlo Tree Search (MCTS) by balancing exploration and exploitation, rather than pre-programmed heuristics. By combining a transformer architecture-based Policy Neural Network (PNN) with MCTS, the agent progressively refines its move selection and win prediction accuracy through iterative self-play. Our model's performance is evaluated by comparing the ELO rating of our model against StockFish, an established chess agent and focussing on both raw performance and overall chess skill. Upon training the model and self-playing our agent against StockFish at strength 6 (which is around ~1950 ELO rating), we achieved a win-loss ratio of 41-44-15 (W-L-D) on the large model and 27-55-18 (W-L-D) on the small model, which is a respectable record and shows that our agent is capable of playing some high-level chess. Ultimately, our work provides insights into the feasibility of applying reinforcement learning with an attention-based transformer architecture to chess engines and lays the groundwork for future optimization.
